{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kCIAcX580UgX"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","path = '/content/gdrive/My Drive/Outsourcing/DACON/MVtec/'\n","\n","import os\n","os.chdir(path)\n","\n","!pip install timm\n","!pip install torchsampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMI_krt-14A9"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from glob import glob\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import cv2\n","\n","import os\n","import timm\n","import random\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from sklearn.metrics import f1_score, accuracy_score\n","import time\n","from torchsampler.imbalanced import ImbalancedDatasetSampler\n","\n","device = torch.device('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibHSA4Lz191r"},"outputs":[],"source":["\n","train_png = sorted(glob('open/train/*.png'))\n","test_png = sorted(glob('open/test/*.png'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","train_y = pd.read_csv(\"open/train_df.csv\")\n","\n","train_labels = train_y[\"label\"]\n","\n","label_unique = sorted(np.unique(train_labels))\n","label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n","\n","train_labels = [label_unique[k] for k in train_labels]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def img_load(path):\n","    img = cv2.imread(path)[:,:,::-1]\n","    img = cv2.resize(img, (512, 512))\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_imgs = [img_load(m) for m in tqdm(train_png)]\n","test_imgs = [img_load(n) for n in tqdm(test_png)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Custom_dataset(Dataset):\n","    def __init__(self, img_paths, labels, mode='train'):\n","        self.img_paths = img_paths\n","        self.labels = labels\n","        self.mode=mode\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img = self.img_paths[idx]\n","\n","        if self.mode=='train':\n","            augmentation = random.randint(0,2)\n","            if augmentation==1:\n","                img = img[::-1].copy()\n","            elif augmentation==2:\n","                img = img[:,::-1].copy()\n","\n","        img = transforms.ToTensor()(img)\n","        if self.mode=='test':\n","            pass\n","\n","        label = self.labels[idx]\n","        return img, label\n","\n","    def get_labels(self):\n","        return self.labels\n","    \n","\n","class Network(nn.Module):\n","    def __init__(self):\n","        super(Network, self).__init__()\n","        self.model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=88)\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        return x\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 32\n","epochs = 120\n","\n","# Train\n","train_dataset = Custom_dataset(np.array(train_imgs), np.array(train_labels), mode='train')\n","train_loader = DataLoader(train_dataset, batch_size=batch_size,\n","                          sampler=ImbalancedDatasetSampler(train_dataset))\n","\n","# Test\n","test_dataset = Custom_dataset(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n","\n","\n","def score_function(real, pred):\n","    score = f1_score(real, pred, average=\"macro\")\n","    return score\n"]},{"cell_type":"markdown","metadata":{},"source":["# ImbalancedSampler Debugging"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ImbalancedSampler Debugging ------------------------------------\n","invert_label_unique = {value:key for key, value in label_unique.items()}\n","df = pd.read_csv(\"open/train_df.csv\")\n","\n","for idx, sample_data in enumerate(train_loader):\n","    if idx == 1: break\n","    # print(sample_data)\"\"\n","    x = torch.tensor(sample_data[0], dtype=torch.float32, device=device)\n","    y = torch.tensor(sample_data[1], dtype=torch.long, device=device)\n","    \n","    label_names = []\n","    print(f\"{'sampled sample_data label':<30} | {'Total Count'}\")\n","    print(\"-\" * 50)\n","    for sample in y:\n","        sample = sample.item()\n","        label_name = invert_label_unique[sample]\n","        label_names.append(label_name)\n","\n","        sampled_label_cnt = len(df[df['label'] == label_name])\n","        print(f\"{label_name:<30} | {sampled_label_cnt:>10}\")\n","    print(label_names)\n","    print(y)\n","# ---------------------------------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = Network().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","scaler = torch.cuda.amp.GradScaler()\n","\n","\n","best=0\n","train_f1_list = []\n","for epoch in range(epochs):\n","    start=time.time()\n","    train_loss = 0\n","    train_pred=[]\n","    train_y=[]\n","    model.train()\n","    for batch in (train_loader):\n","        optimizer.zero_grad()\n","        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n","        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n","        with torch.cuda.amp.autocast():\n","            pred = model(x)\n","        loss = criterion(pred, y)\n","\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        train_loss += loss.item()/len(train_loader)\n","        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n","        train_y += y.detach().cpu().numpy().tolist()\n","\n","\n","    train_f1 = score_function(train_y, train_pred)\n","    train_f1_list.append(train_f1)\n","    # print(train_f1_list)\n","    if max(train_f1_list) > train_f1_list[-1]:\n","        torch.save(model.state_dict(), \"./model/\"+str(epoch)+\".pt\")\n","    \n","    TIME = time.time() - start\n","    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n","    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMXIil0RKqf5NddwOUNdELB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
